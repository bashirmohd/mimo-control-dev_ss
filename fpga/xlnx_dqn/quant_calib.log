No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m
Net(
  (cnn_layers): Sequential(
    (0): Conv2d(2, 24, kernel_size=(2, 2), stride=(1, 1))
    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): CELU(alpha=1.0)
    (3): Conv2d(24, 38, kernel_size=(2, 2), stride=(1, 1))
    (4): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): CELU(alpha=1.0)
    (6): Conv2d(38, 66, kernel_size=(2, 2), stride=(1, 1))
    (7): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): CELU(alpha=1.0)
  )
  (linear_layers): Sequential(
    (0): Linear(in_features=264, out_features=60, bias=True)
    (1): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): CELU(alpha=1.0)
    (3): Linear(in_features=60, out_features=8, bias=True)
    (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): CELU(alpha=1.0)
    (6): Linear(in_features=8, out_features=8, bias=False)
  )
)
QUANT_MODE:  calib
RAND_IN:  tensor([[[[-0.0893, -1.0132,  0.9222, -2.2509,  0.7524],
          [ 0.1269, -0.5720, -0.3101,  1.6919,  0.6075],
          [-2.2520,  0.1829,  1.4213,  0.5180,  1.0880],
          [ 2.1988, -0.9527,  0.2185, -1.8122, -0.3091],
          [-1.7292, -0.0500,  1.0752,  0.4195,  1.4724]],

         [[-0.5561, -0.0573,  1.9160, -0.4494, -0.5169],
          [-0.2350, -0.4295,  1.7625,  0.1224, -0.1423],
          [-1.8433,  0.3519,  1.9222, -0.0385,  2.1240],
          [-0.8718, -0.4821,  0.5793, -0.8545, -0.2430],
          [ 1.6927,  1.6344,  1.0040,  1.1854, -0.8289]]],


        [[[-1.0113,  0.6557,  0.2360, -0.5444, -0.2381],
          [-0.9053,  0.5800, -0.7697,  1.7709, -1.4797],
          [-1.7743, -1.1570,  0.0192, -0.4124,  1.4741],
          [-0.1405, -1.1365,  0.1717,  1.0499,  1.0538],
          [-0.2323,  1.1559,  2.2277,  0.6150,  0.7232]],

         [[ 0.6358,  0.7734,  0.9908,  0.3716,  0.4068],
          [-0.5534, -0.9439, -0.3677, -0.4125,  0.6034],
          [ 0.0547,  0.4883,  0.0925,  0.1677,  1.4641],
          [-0.5289, -1.1238,  0.0028,  0.4088, -0.3176],
          [ 0.0325,  1.1818,  0.2012, -0.4441,  0.9158]]],


        [[[-0.7746,  0.0334, -1.4749,  0.0984, -0.3695],
          [ 0.5662, -0.2801,  0.6713,  0.0272, -0.1910],
          [ 0.2966,  0.1348, -0.1896, -0.9320,  0.7558],
          [ 1.6408, -1.6509,  1.1172,  1.2378,  0.8611],
          [-0.0684, -0.7761,  0.6660, -0.1077, -0.4725]],

         [[-0.2941, -1.8483, -1.7876,  0.6834,  0.3749],
          [ 0.0202, -0.1538, -0.3924, -0.2792, -0.8096],
          [ 0.4604, -0.2748,  0.6588,  0.2752,  0.2665],
          [ 0.5333, -0.1617, -0.7123, -1.1261, -1.9522],
          [-0.0448,  0.6972,  0.8520,  0.1398, -0.9080]]],


        ...,


        [[[ 0.7336, -0.8115,  1.4013,  0.7198, -0.4396],
          [ 0.9956,  0.1705, -0.1701,  1.4513, -0.3873],
          [ 0.3219, -0.3411,  2.3144, -0.9047,  0.2370],
          [ 0.1479,  0.3306, -0.9675, -0.7480, -0.8668],
          [-0.2977,  0.4560, -1.6515, -1.0811, -0.8434]],

         [[-0.5268, -0.0311,  0.5645, -1.0443,  0.5286],
          [ 0.1356, -1.1202, -0.1329, -0.2399,  0.6286],
          [-0.0642,  0.7715, -0.0121,  0.1198, -0.9418],
          [ 1.1010,  0.1925, -0.7735,  1.2474,  0.8975],
          [ 1.8472,  1.5049, -0.1698,  1.3637,  1.7446]]],


        [[[-0.6891,  1.4370, -1.2858, -0.7282,  1.8205],
          [-0.4956, -0.8605, -0.6976, -0.7125, -0.6348],
          [-0.4327, -0.7838, -1.6734,  0.2832, -0.2016],
          [ 0.0115, -0.5981, -0.6349,  0.2435,  2.1451],
          [ 1.2962, -1.1598, -0.9475, -1.6210, -0.0617]],

         [[-0.6514, -0.0306, -0.0555,  0.5627,  0.5001],
          [-0.1406,  1.9121,  0.8125, -0.3578,  0.2102],
          [-0.5866,  0.3259,  1.7271,  1.4085,  0.8527],
          [-1.1639, -0.6555, -0.9641,  0.0205, -0.1536],
          [-0.1072,  1.0897,  0.1988,  0.0228, -0.2097]]],


        [[[ 0.9835, -0.7290, -1.7021, -1.4724,  0.9326],
          [ 0.2054, -0.5005, -0.6427, -0.0550, -0.8070],
          [ 0.7214, -1.9203,  0.8852,  0.7479,  1.0188],
          [ 0.0228, -0.1087, -1.7699, -0.5857, -0.0840],
          [-1.3429,  0.2041,  1.6133,  0.0594, -1.9494]],

         [[ 0.0563, -0.2691,  0.3267,  1.5419,  0.5272],
          [-1.5250, -0.1285,  0.1092, -1.8103, -0.3190],
          [ 0.1359, -0.6695,  1.3442,  0.7962, -0.1325],
          [ 1.2385,  0.1599,  1.1485, -0.3076, -0.5075],
          [ 0.2324, -0.0775, -1.2463, -0.2139, -1.2736]]]])
RAND_IN SHAPE:  torch.Size([80, 2, 5, 5])
QUANT_MODEL:  quant_model

[0;33m[VAIQ_WARN]: CUDA is not available, change device to CPU[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing Net...[0m

[0;33m[VAIQ_WARN]: The quantizer ignores new op `aten::celu` by default.[0m

[0;32m[VAIQ_NOTE]: You can make these new ops quantizable by add them to custom_quant_ops, e.g. quantizer= torch_quantizer(..., custom_quant_ops=['aten::celu',...])[0m

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_model/Net.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m

-----------------------------------------
Model successfully quantized. Evaluating quantized model...
-----------------------------------------

[ 1/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.02364
[ 2/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.02364
[ 3/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.02364
[ 4/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.02364
[ 5/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.02364
[ 6/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.02364
[ 7/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.02364
[ 8/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.02364
[ 9/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.02364
[10/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00236
[11/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00236
[12/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00236
[13/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00236
[14/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00236
[15/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00236
[16/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00236
[17/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00236
[18/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00024
[19/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00024
[20/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00024
[21/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00024
[22/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00024
[23/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00024
[24/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00024
[25/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00024
[26/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00002
[27/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00002
[28/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00002
[29/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00002
[30/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00002
[31/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00002
[32/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00002
[33/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00002
[34/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00000
[35/43] train_loss: 10.61 deg, valid_loss: 10.61 deg, lr: 0.00000
